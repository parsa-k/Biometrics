{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               file_path  label\n",
      "0   ./data\\01\\0_01_0.wav      0\n",
      "1   ./data\\01\\0_01_1.wav      0\n",
      "2  ./data\\01\\0_01_10.wav      0\n",
      "3  ./data\\01\\0_01_11.wav      0\n",
      "4  ./data\\01\\0_01_12.wav      0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the data directory\n",
    "data_dir = './data'\n",
    "\n",
    "# List all folders (people) in the data directory\n",
    "people_folders = os.listdir(data_dir)\n",
    "\n",
    "# Initialize an empty list to hold the data\n",
    "data = []\n",
    "\n",
    "# Loop through each folder (person)\n",
    "for label, person_folder in enumerate(people_folders):\n",
    "    # Path to the current person's folder\n",
    "    person_folder_path = os.path.join(data_dir, person_folder)\n",
    "    \n",
    "    # Ensure it's a directory\n",
    "    if os.path.isdir(person_folder_path):\n",
    "        # List all audio files in the current person's folder\n",
    "        audio_files = os.listdir(person_folder_path)\n",
    "        \n",
    "        # Loop through each audio file and create a record with the label (person) and filename\n",
    "        for audio_file in audio_files:\n",
    "            # Create the file path\n",
    "            file_path = os.path.join(person_folder_path, audio_file)\n",
    "            # Append the filename and label to the data list\n",
    "            data.append([file_path, label])\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "df = pd.DataFrame(data, columns=['file_path', 'label'])\n",
    "\n",
    "# Show the first few records\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms\n",
    "\n",
    "# Step 1: Read data and prepare labels and file paths\n",
    "def get_subfolder_names(main_folder_path):\n",
    "    file_names = []\n",
    "    subfolder_names = []\n",
    "    for folder_ID in os.listdir(main_folder_path):\n",
    "        subfolder_path = os.path.join(main_folder_path, folder_ID)\n",
    "        for file_ID in os.listdir(subfolder_path):\n",
    "            if os.path.isdir(subfolder_path):\n",
    "                subfolder_names.append(folder_ID)\n",
    "            if os.path.isfile(os.path.join(subfolder_path, file_ID)):\n",
    "                file_names.append(file_ID)\n",
    "    return subfolder_names, file_names\n",
    "\n",
    "# Load all subfolder and file names into a DataFrame\n",
    "main_folder_path = \"C:/Users/ryan_/OneDrive/Documents/USF/Courses/Fall 2024/Mobile Biometrics/Project/Voice Recognition/data\"\n",
    "subfolders, file_names = get_subfolder_names(main_folder_path)\n",
    "data = {'Label': subfolders, 'File Name': file_names}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 2: Generate spectrograms and split the dataset\n",
    "def add_noise_to_audio(audio, noise_factor):\n",
    "    \"\"\"\n",
    "    Adds Gaussian noise to the audio signal.\n",
    "    \n",
    "    Parameters:\n",
    "    - audio: The raw audio signal (numpy array)\n",
    "    - noise_factor: Standard deviation of the Gaussian noise to be added\n",
    "    \n",
    "    Returns:\n",
    "    - Noisy audio signal (numpy array)\n",
    "    \"\"\"\n",
    "    noise = np.random.normal(0, noise_factor, audio.shape)\n",
    "    noisy_audio = audio + noise\n",
    "    # Clip the noisy audio to prevent values from going out of the valid range [-1, 1]\n",
    "    noisy_audio = np.clip(noisy_audio, -1, 1)\n",
    "    return noisy_audio\n",
    "\n",
    "\n",
    "class VoiceDataset(Dataset):\n",
    "    def __init__(self, df, main_folder_path, transform=None, noise_factor=0.0001):\n",
    "        self.df = df\n",
    "        self.main_folder_path = main_folder_path\n",
    "        self.transform = transform\n",
    "        self.noise_factor = noise_factor  # Parameter to control the amount of noise\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        label = row['Label']\n",
    "        file = row['File Name']\n",
    "\n",
    "        filepath = os.path.join(self.main_folder_path, label, file)\n",
    "\n",
    "        label = int(label)-1\n",
    "        y, sr = librosa.load(filepath, sr=None)\n",
    "\n",
    "        # Ensure the audio is at least 1 second long\n",
    "        target_length = sr * 1  # 1 second of audio\n",
    "        if len(y) < target_length:\n",
    "            y = np.pad(y, (0, target_length - len(y)), mode='constant')\n",
    "        else:\n",
    "            y = y[:target_length]\n",
    "\n",
    "        # Add noise to the audio signal\n",
    "        noisy_audio = add_noise_to_audio(y, self.noise_factor)\n",
    "\n",
    "        # Perform Short-Time Fourier Transform (STFT)\n",
    "        D = librosa.stft(noisy_audio)  # Use noisy audio here\n",
    "        DB = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
    "\n",
    "        # Convert the spectrogram to a tensor and normalize if needed\n",
    "        spectrogram = torch.tensor(DB, dtype=torch.float32)\n",
    "\n",
    "        # Normalize the spectrogram (ResNet18 normalization)\n",
    "        spectrogram = (spectrogram - spectrogram.mean()) / spectrogram.std()\n",
    "        spectrogram = torch.nn.functional.interpolate(spectrogram.unsqueeze(0).unsqueeze(0), size=(224, 224), mode='bilinear', align_corners=False).squeeze(0).squeeze(0)\n",
    "\n",
    "\n",
    "        # If a transform is provided (e.g., resizing or additional augmentations)\n",
    "        #if self.transform:\n",
    "         #   spectrogram = self.transform(spectrogram)\n",
    "\n",
    "        #    print('spectrogram shape is', spectrogram.shape)\n",
    "\n",
    "        return spectrogram, label\n",
    "\n",
    "# Step 3: Split data by person and create train, validation, test sets\n",
    "# Create a mapping of subfolders (people) to file paths\n",
    "# Step 3: Split data by person and create train, validation, test sets\n",
    "# Create a mapping of subfolders (people) to file paths\n",
    "people_data = {}\n",
    "for _, row in df.iterrows():\n",
    "    label = row['Label']\n",
    "    file = row['File Name']\n",
    "    filepath = os.path.join(main_folder_path, label, file)\n",
    "    \n",
    "    if label not in people_data:\n",
    "        people_data[label] = []\n",
    "    people_data[label].append(filepath)\n",
    "\n",
    "\n",
    "# Use all the available 40 people (folders)\n",
    "# Since labels are already the folder names, we do not need a separate mapping\n",
    "labels = list(people_data.keys())\n",
    "\n",
    "# Split files for each person into train, val, and test sets\n",
    "\n",
    "train_files, val_test_files = [], []\n",
    "train_label, val_test_label = [], []\n",
    "for person in labels:\n",
    "    person_files = people_data[person]\n",
    "    train_size = int(0.8 * len(person_files))\n",
    "    \n",
    "    # Split data for each person\n",
    "    train_files.extend(person_files[:train_size])\n",
    "    train_label.extend([person]*(train_size))  # Use folder name (string) as label\n",
    "    val_test_files.extend(person_files[train_size:])\n",
    "    val_test_label.extend([person]*(len(person_files) - train_size))  # Use folder name (string) as label\n",
    "    \n",
    "# Further split the remaining 10% into val and test sets\n",
    "val_files, test_files, val_label, test_label = train_test_split(val_test_files, val_test_label, test_size=0.5)\n",
    "\n",
    "# Combine the files into DataFrames using the label directly\n",
    "train_df = pd.DataFrame({'File Name': train_files, 'Label': train_label})\n",
    "val_df = pd.DataFrame({'File Name': val_files, 'Label': val_label})\n",
    "test_df = pd.DataFrame({'File Name': test_files, 'Label': test_label})\n",
    "\n",
    "\n",
    "\n",
    "# Step 4: Create DataLoaders for training, validation, and test sets\n",
    "# Define a transform for ResNet18 preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert grayscale to 3 channels\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize for ResNet\n",
    "])\n",
    "\n",
    "train_dataset = VoiceDataset(train_df, main_folder_path, transform=transform)\n",
    "val_dataset = VoiceDataset(val_df, main_folder_path, transform=transform)\n",
    "test_dataset = VoiceDataset(test_df, main_folder_path, transform=transform)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# The datasets and dataloaders are now ready to be used with a model for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 224, 224])\n",
      "torch.Size([64])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "for images , labels in train_loader:\n",
    "    print(images.shape)\n",
    "    print(labels.shape)\n",
    "    print(type(labels[0]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def add_noise_to_spectrogram(spectrogram, noise_factor):\n",
    "    \"\"\"\n",
    "    Adds Gaussian noise to a spectrogram.\n",
    "    \n",
    "    Parameters:\n",
    "    - spectrogram: The input spectrogram (numpy array)\n",
    "    - noise_factor: Standard deviation of the Gaussian noise to be added\n",
    "    \n",
    "    Returns:\n",
    "    - Noisy spectrogram (numpy array)\n",
    "    \"\"\"\n",
    "    noise = np.random.normal(0, noise_factor, spectrogram.shape)\n",
    "    noisy_spectrogram = spectrogram + noise\n",
    "    # Clip the noisy spectrogram values to a valid range (0 to max value)\n",
    "    noisy_spectrogram = np.clip(noisy_spectrogram, 0, 1)  # Ensure values are within [0, 1] range\n",
    "    return noisy_spectrogram\n",
    "\n",
    "\n",
    "def plot_spectrograms(dataloader, title, n_images=16, noise_factor=0.001):\n",
    "    # Set up the plot\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(12, 12))  # 4x4 grid of subplots\n",
    "    axes = axes.flatten()  # Flatten the axes for easy iteration\n",
    "\n",
    "    # Iterate through the dataloader and plot 16 images\n",
    "    for i, (spectrograms, labels) in enumerate(dataloader):\n",
    "        if i >= 1:  # We only need the first batch\n",
    "            break\n",
    "\n",
    "        # Randomly select 16 spectrograms from the batch\n",
    "        random_indices = np.random.choice(len(spectrograms), size=n_images, replace=False)\n",
    "\n",
    "        for j, idx in enumerate(random_indices):\n",
    "            ax = axes[j]\n",
    "            spec = spectrograms[idx].numpy()\n",
    "\n",
    "            # Normalize the spectrogram to the range [0, 1] for better display\n",
    "            norm_spec = (spec - np.min(spec)) / (np.max(spec) - np.min(spec) + 1e-8)\n",
    "\n",
    "            # Add noise to the spectrogram\n",
    "            noisy_spec = add_noise_to_spectrogram(norm_spec, noise_factor)\n",
    "\n",
    "            # Apply log scaling (logarithmic transformation) for better visualization\n",
    "            log_spec = np.log(noisy_spec + 1e-8)  # Adding a small epsilon to avoid log(0)\n",
    "\n",
    "            # Plot the spectrogram\n",
    "            ax.imshow(spec, aspect='auto', cmap='inferno', origin='lower')\n",
    "            ax.set_title(f'Label: {labels[idx]}')\n",
    "            ax.axis('off')  # Hide axis ticks and labels\n",
    "\n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Step 1: Plot 16 images for one person in the training set with noise\n",
    "plot_spectrograms(train_loader, \"Training Set: 16 Spectrograms with Noise\")\n",
    "\n",
    "# Step 2: Plot 16 images for one person in the validation set with noise\n",
    "plot_spectrograms(val_loader, \"Validation Set: 16 Spectrograms with Noise\")\n",
    "\n",
    "# Step 3: Plot 16 images for one person in the test set with noise\n",
    "plot_spectrograms(test_loader, \"Test Set: 16 Spectrograms with Noise\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOdel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the pre-trained ResNet18 model\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Modify the first convolutional layer to accept 1 channel instead of 3 (for grayscale images)\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "# Modify the final fully connected layer to match the number of labels (for CelebA, you have N unique labels)\n",
    "num_labels = 40  # This is the number of unique labels after filtering\n",
    "model.fc = nn.Linear(model.fc.in_features, num_labels)\n",
    "\n",
    "# Freeze all layers except the final fully connected layer\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the last fully connected layer to allow it to be trained\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Move the model to the appropriate device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Multi-class classification loss function\n",
    "optimizer = optim.SGD(model.fc.parameters(), lr=0.001, momentum=0.9)  # Only optimize the last layer\n",
    "\n",
    "# Number of epochs for training\n",
    "num_epochs = 10\n",
    "\n",
    "# Training and validation loop\n",
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs=10):\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        # Train loop\n",
    "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Train)\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Reshape images to have 1 channel (grayscale)\n",
    "            images = images.unsqueeze(1)  # Now the shape is [batch_size, 1, 224, 224]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "            total_train += labels.size(0)\n",
    "\n",
    "        # Calculate training accuracy\n",
    "        train_accuracy = correct_train / total_train\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        correct_valid = 0\n",
    "        total_valid = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in valid_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                images = images.unsqueeze(1)  # Now the shape is [batch_size, 1, 224, 224]\n",
    "                \n",
    "                outputs = model(images)\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_valid += (predicted == labels).sum().item()\n",
    "                total_valid += labels.size(0)\n",
    "\n",
    "        # Calculate validation accuracy\n",
    "        valid_accuracy = correct_valid / total_valid\n",
    "\n",
    "        # Print loss and accuracy for each epoch\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}, \"\n",
    "              f\"Train Accuracy: {train_accuracy:.4f}, Validation Accuracy: {valid_accuracy:.4f}\")\n",
    "\n",
    "        # Save the model if validation accuracy is better\n",
    "        if valid_accuracy > best_accuracy:\n",
    "            best_accuracy = valid_accuracy\n",
    "            torch.save(model.state_dict(), \"best_resnet18.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 (Train): 100%|██████████| 250/250 [08:52<00:00,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 3.4934, Train Accuracy: 0.0825, Validation Accuracy: 0.1190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 (Train): 100%|██████████| 250/250 [09:56<00:00,  2.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50, Loss: 3.2200, Train Accuracy: 0.1459, Validation Accuracy: 0.1730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 (Train):  19%|█▉        | 48/250 [01:49<07:41,  2.29s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[190], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Call the training function with the training and validation dataloaders\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[189], line 54\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, valid_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m     51\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Now the shape is [batch_size, 1, 224, 224]\u001b[39;00m\n\u001b[0;32m     53\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 54\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     56\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\ryan_\\anaconda3\\envs\\py311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ryan_\\anaconda3\\envs\\py311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ryan_\\anaconda3\\envs\\py311\\Lib\\site-packages\\torchvision\\models\\resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ryan_\\anaconda3\\envs\\py311\\Lib\\site-packages\\torchvision\\models\\resnet.py:274\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    271\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool(x)\n\u001b[0;32m    273\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[1;32m--> 274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n\u001b[0;32m    276\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer4(x)\n",
      "File \u001b[1;32mc:\\Users\\ryan_\\anaconda3\\envs\\py311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ryan_\\anaconda3\\envs\\py311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ryan_\\anaconda3\\envs\\py311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ryan_\\anaconda3\\envs\\py311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ryan_\\anaconda3\\envs\\py311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ryan_\\anaconda3\\envs\\py311\\Lib\\site-packages\\torchvision\\models\\resnet.py:92\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m     90\u001b[0m     identity \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m---> 92\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[0;32m     94\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[1;32mc:\\Users\\ryan_\\anaconda3\\envs\\py311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ryan_\\anaconda3\\envs\\py311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ryan_\\anaconda3\\envs\\py311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ryan_\\anaconda3\\envs\\py311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Call the training function with the training and validation dataloaders\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    # Load the best model weights if saved\n",
    "    model.load_state_dict(torch.load(\"best_resnet18.pth\"))\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # No gradient computation during evaluation\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Get the predictions\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    # Compute accuracy\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=range(num_labels), yticklabels=range(num_labels))\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with your test dataloader\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            #outputs = torch.softmax(outputs, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(torch.max(outputs, 1)[1])):\n",
    "    formatted_tensor = [f\"{val:.2f}\" for val in outputs[i].tolist()]\n",
    "    print('True:',labels[i], '-- predicted:', torch.max(outputs, 1)[1][i], formatted_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Placeholder for data to save\n",
    "data_to_save = []\n",
    "\n",
    "# Loop through your data loader\n",
    "for images, labels in test_loader:\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    outputs = model(images)\n",
    "\n",
    "    outputs = torch.softmax(outputs, dim=1) # apply sofmax\n",
    "    \n",
    "    for i in range(len(outputs)):\n",
    "        # Extract relevant information\n",
    "        true_label = labels[i].item()\n",
    "        predicted_label = torch.argmax(outputs[i]).item()\n",
    "        formatted_tensor = [f\"{val:.4f}\" for val in outputs[i].tolist()]\n",
    "        \n",
    "        # Add to data list as a single row\n",
    "        row = [true_label, predicted_label] + formatted_tensor\n",
    "        data_to_save.append(row)\n",
    "    \n",
    "\n",
    "# Define column names\n",
    "num_elements = len(formatted_tensor)  # Number of elements in the tensor\n",
    "columns = [\"True Label\", \"Predicted Label\"] + [f\"{i}\" for i in range(num_elements)]\n",
    "\n",
    "# Create a DataFrame and save it to a CSV file\n",
    "df = pd.DataFrame(data_to_save, columns=columns)\n",
    "df.to_csv(\"results.csv\", index=False)\n",
    "\n",
    "print(\"Data saved to 'results.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
